#!/usr/bin/env python3

import collections
import torch
import warnings

from operator import itemgetter
from typing import DefaultDict, Dict, List, Tuple


class Vocabulary(object):
    """Represents a collection of tokens with their mapped ID and frequencies.
    """
    PAD_ID = 0
    PAD_TOKEN = '<PAD>'
    GO_ID = 1
    GO_TOKEN = '<GO>'
    EOS_ID = 2
    EOS_TOKEN = '<EOS>'
    UNK_ID = 3
    UNK_TOKEN = '<UNK>'
    NUM_SPECIAL_TOKENS = 4

    @staticmethod
    def _special_token_freq() -> List[Tuple[str, int]]:
        return [
            (Vocabulary.PAD_TOKEN, -1),
            (Vocabulary.GO_TOKEN, -1),
            (Vocabulary.EOS_TOKEN, -1),
            (Vocabulary.UNK_TOKEN, -1),
        ]

    def __init__(self):
        self._initialized = False
        self._token_to_id: Dict[str, int] = {}
        self._id_to_token_freq: List[Tuple[str, int]] = []

    def _check_init(self):
        if not self._initialized:
            raise RuntimeError(
                'Vocabulary has not been initialized. Call load_serialized() '
                'or load_tokenized_text() before using.'
            )

    def _check_id(self, id):
        if (id > self.vocab_size()):
            raise IndexError(
                'Index: {} out of bound for vocab size: {}'.format(
                    id,
                    self.vocab_size(),
                )
            )

    def id_to_token(self, id: int) -> str:
        self._check_init()
        self._check_id(id)
        return self._id_to_token_freq[id][0]

    def id_to_freq(self, id: int) -> int:
        self._check_init()
        self._check_id(id)
        return self._id_to_token_freq[id][1]

    def token_to_id(self, token: str) -> int:
        self._check_init()
        return (
            self._token_to_id[token]
            if token in self._token_to_id else Vocabulary.UNK_ID
        )

    def token_to_freq(self, token: str) -> int:
        self._check_init()
        if (token not in self._token_to_id):
            warnings.warn(
                'Trying to retrieve freq for out-of-vocab token: "{}"'.format(
                    token,
                ),
                RuntimeWarning,
            )
            return -1
        return self._id_to_token_freq[self._token_to_id[token]][1]

    def vocab_size(self) -> int:
        self._check_init()
        return len(self._id_to_token_freq)

    def save(self, file_path: str):
        """Serializes this vocab to a file.
        """
        torch.save(self._id_to_token_freq, file_path)

    def _init_token_to_id(self, id_to_token_freq: List[Tuple[str, int]]):
        self._token_to_id = {}
        for i in range(len(id_to_token_freq)):
            token = id_to_token_freq[i][0]
            self._token_to_id[token] = i

    def load_serialized(self, file_path: str):
        """Initializes vocab from a serialized file generated by save().
        """
        self._initialized = False
        self._id_to_token_freq = torch.load(file_path)
        special_token_freq = Vocabulary._special_token_freq()
        if (self._id_to_token_freq[:Vocabulary.NUM_SPECIAL_TOKENS] !=
                special_token_freq):
            raise RuntimeError(
                'Invalid serialized vocabulary - missing special tokens. '
                'Expected: {}, but found: {}'.format(
                    special_token_freq,
                    self._id_to_token_freq[:Vocabulary.NUM_SPECIAL_TOKENS],
                )
            )

        self._init_token_to_id(self._id_to_token_freq)
        self._initialized = True

    def load_tokenized_text(self, file_path: str, top_n: int = None):
        """Initializes vocab from a tokenized text file.

        The file should contain tokenized words separated by whitespaces.
        If top_n is specified, only the top_n most frequent words are kept in
        the vocab; if not specified, all words are kept.
        """
        self._initialized = False
        token_freqs: DefaultDict[str, int] = collections.defaultdict(int)
        with open(file_path, 'r') as f:
            for sentence in f:
                tokens = sentence.strip().split()
                for token in tokens:
                    token_freqs[token] += 1

        token_freqs_list = list(token_freqs.items())
        token_freqs_list.sort(
            # Sorts by frequency first, then (reverse) alphabetical.
            key=itemgetter(1, 0),
            reverse=True,
        )

        self._id_to_token_freq = self._special_token_freq()
        self._id_to_token_freq.extend(token_freqs_list[:top_n])
        self._init_token_to_id(self._id_to_token_freq)
        self._initialized = True
